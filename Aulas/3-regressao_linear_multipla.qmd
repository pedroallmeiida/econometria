---
title: "Regressão Linear Múltipla"
title-slide-attributes:
  data-background-image: Marca_UEPB.png
  data-background-size: contain
  data-background-opacity: "0.15"
author: "Professor: Pedro M. Almeida-Junior"
institute: "Universidade Estadual da Paraíba"
format: 
  revealjs: 
    width: 1000
    margin: 0.1
    logo: Marca_UEPB.png
    theme: [default, hscroll.scss]
    transition: slide
    background-transition: fade
    smaller: true
    scrollable: true
knitr:
  opts_chunk:
    message: false
    warning: false
css: hscroll.scss
editor: 
  markdown: 
    wrap: 72
---

# Modelos de regressão Linear Múltipla

## Modelos de regressão Linear Múltipla

<hr>

<br/>

Seja $Y$ a **variável resposta**, e seja $X_1, \ldots, X_n$ as
**variáveís explicativas**. O modelo de regressão linear múltipla
descreve a variável $Y$ através de duas ou mais variáveis explicativas.
Neste sentido, o modelo é definido como:

<br/>

$$
Y_i = \beta_0 + \beta_1X_{i1} + \cdots \beta_pX_{ip} + \epsilon_i,  \quad i = 1, \ldots, n. 
$$

<br/>

em que o parâmetro $\beta_0$ é o intercepto e os parâmetros
$\beta_1, \ldots, \beta_p$ são os coeficiente de regressão.
$X_{i1}, \ldots, X_{in}$ são as variáveis explicativas e $\epsilon_i$ é
um erro aleatório para $i = 1, \ldots,n$.

## Suposições do erro:

<hr>

<br/>

. . .

1.  $\mathbb{E}(\epsilon_i) = 0$ e $\text{Var}(\epsilon_i) = \sigma^2$,
    para todo $i$ e $0 < \sigma^2 < \infty$.

. . .

2.  $\text{Cov}( \epsilon_i, \epsilon_s ) = 0, \forall i \neq s$.

. . .

3.  $\epsilon_i$ tem distribuição normal.

::: {style="height:800px; font-size:23px; margin-left: 5px; margin-right: 5px"}
## Forma Matricial

Podemos rescrever o modelo de regressão linear múltipla de forma
matricial. Neste caso, temos:

$$
\boldsymbol Y = \boldsymbol{X\beta} + \boldsymbol \epsilon
$$ em que,

$$
\mathbf{Y} =
\begin{bmatrix}
y_1 \\
y_2 \\
y_3 \\
\vdots \\
y_n
\end{bmatrix}
, \quad
\mathbf{X} =
\begin{bmatrix}
1 & x_{11} & x_{12} & \dots & x_{1p} \\
1 & x_{21} & x_{22} & \dots & x_{2p} \\
1 & x_{31} & x_{32} & \dots & x_{3p} \\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_{n1} & x_{n2} & \dots & x_{np}
\end{bmatrix},
\boldsymbol{\beta} =
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\beta_2 \\
\vdots \\
\beta_p
\end{bmatrix}
\quad \text{e} \quad 
\boldsymbol{\varepsilon} =
\begin{bmatrix}
\varepsilon_1 \\
\varepsilon_2 \\
\varepsilon_3 \\
\vdots \\
\varepsilon_n
\end{bmatrix}
$$

<br/>

em que
$\boldsymbol \epsilon \sim N( \boldsymbol 0, \sigma^2 \boldsymbol I )$,
sendo $\boldsymbol 0$ o vetor de nulos e $\boldsymbol I$ a matriz
identidade.
:::

## [Distribuição da variável resposta Y]{style="font-size: 40px;"}

<br/>

Considere o modelo de regressão linear múltipla em forma matricial.
Então a distribuição de probabilidade de $\boldsymbol Y$, correspondente
a matriz de covariável $\boldsymbol X$, é dada por:

<br/>

<br/>

$$
Y \sim N(   \boldsymbol{X\beta}, \sigma^2 \boldsymbol I). 
$$

::: {style="height:600px; font-size:25px; margin-left: 5px; margin-right: 5px"}
## 

**Prova:** Sob o modelo de regressão linear múltipla, $\boldsymbol Y$ é
a soma de uma constante, $\boldsymbol{X\beta}$, com um vetor de
variáveis aleatórias $\boldsymbol \epsilon$, com distribuição
$N(\boldsymbol 0, \sigma^2\boldsymbol I)$. Dessa forma:

. . .

$$
\begin{align}
\mathbb{E}[Y \mid x] &= \mathbb{E}[ \boldsymbol{X\beta} + \boldsymbol \epsilon ] \\
&=\mathbb{E}[ \boldsymbol{X\beta} ]  + \mathbb{E}[\epsilon ]  \\
&= \boldsymbol{X\beta} + \mathbb{E}[\epsilon ] \\
&= \boldsymbol{X\beta} + \boldsymbol 0 \\
&= \boldsymbol{X\beta}.
\end{align}
$$

. . .

$$
\begin{align}
\mathbb{Var}[Y \mid x] &= \mathbb{Var}[ \boldsymbol{X\beta} + \boldsymbol \epsilon ] \\
&=\mathbb{Var}[ \boldsymbol{X\beta}] + \mathbb{Var}[\boldsymbol \epsilon ]  \\
&= \boldsymbol 0 + \mathbb{Var}[ \boldsymbol\epsilon ] \\
&= \sigma^2\boldsymbol I.
\end{align}
$$

. . .

Portanto,
$\boldsymbol Y \sim N( \boldsymbol{X\beta}, \sigma^2\boldsymbol I )$.
:::

## Esimação de Mínimos Quadrados

<br/>

A solução para a estimativa dos coeficientes pelo método dos mínimos
quadrados ordinários (MQO) é dada por:

<br/>

$$
\hat{\boldsymbol{\beta}} = (\mathbf{X}^{\top} \mathbf{X})^{-1} \mathbf{X}^{\top} \mathbf{Y}
$$

<br/>

desde que $\mathbf{X}^T \mathbf{X}$ seja invertível.

### [Distribuição dos estimadores dos coeficientes da regressão]{style="font-size: 35px;"}

$$
\hat {\boldsymbol \beta} \sim N_{p+1}\left( \boldsymbol{\beta}, \, \sigma^2( \boldsymbol{X^{\top}X})^{-1}    \right)
$$

## Valores preditos de Y

<br/>

Para estimar os valores da variável resposta $\boldsymbol Y$, usamos a
seguinte expressão:

<br/>

$$
\hat{\boldsymbol{Y}} = \boldsymbol{X\hat{\boldsymbol \beta}}  
$$

<br/>

em que o vetor $\hat{\boldsymbol \beta}$ representa os estimadores para
os parâmetros $\hat\beta_0, \ldots, \hat\beta_p$.

## [Escolha entre o modelo linear simples e o modelo linear múltipla]{style="font-size: 35px;"}

<br/>

Suponha que o vetor $\boldsymbol \beta$ é de dimensão $(p+1)$, sendo
$\beta_0$ o termo constante da equação do modelo com
$\boldsymbol \beta = (\beta_0, \beta_1, \ldots, \beta_p)^{\top}$.

<br/>

Para uma primeira etapa na análise estatística visando a escolha entre
os dois modelos, devemos testar as hipóteses:

$$
\begin{align}
H_0&: \, \beta_1 = \cdots = \beta_p = 0 \\
H_1&: \, \text{Pelo menos um diferente}
\end{align}
$$ <br/>

Se rejeitamos $H_0$, concluímos que há contribuição significante de uma
ou mais variáveis regresspras no estudo de $Y$. A estatística do teste
segue uma distribuição F-Snedecor, com $p$ e $(n - p - 1)$ graus de
liberdade.

## Testes para os parâmetros

<br/>

A contribuição de cada variável no modelo é verificada através do teste
t-student, a partir das seguintes hipóteses:

<br/>

$$
\begin{align}
H_0&: \, \beta_k = 0 \\
H_1&: \, \beta_k \neq 0
\end{align}
$$

A estatística para testarmos a hipótese acima é

$$
T_{\beta_k} = \frac{\hat{\beta}_k}{\sqrt{ \widehat{\sigma^2} diag{ (\boldsymbol{X^{\top}X})^{-1} }  }} \sim t_{(n-p-1)}
$$


# Como escolher as variáveis para o modelo ?

## [Seleção de Variáveis na Regressão Múltipla]{style="font-size: 40px;"}

<br/>

<br/>

A seleção de variáveis é uma etapa essencial na construção de modelos de
regressão múltipla, pois permite identificar quais variáveis
independentes são mais relevantes para prever a variável resposta.
Existem três métodos principais para a seleção de variáveis:
**ForwardSelection**, **Backward Elimination** e **Stepwise Selection**.

## [Forward Selection (Seleção para Frente)]{style="font-size: 40px;"}

Nesse método, a seleção começa com um modelo vazio (sem preditores). As
variáveis são adicionadas uma por uma com base em critérios
estatísticos, como o p-valor dos coeficientes ou o critério de
informação de Akaike (AIC). O processo continua até que nenhuma variável
adicional melhore significativamente o modelo.

**Passos:**

1.  Inicia-se com um modelo sem variáveis independentes.

2.  Adiciona-se a variável que tem a maior associação com a variável
    resposta.

3.  Testa-se a significância da variável adicionada (ex.: p-valor ou
    AIC).

4.  Repete-se o processo até que nenhuma variável adicional melhore
    significativamente o modelo.

## [Backward Elimination (Eliminação para Trás)]{style="font-size: 40px;"}

Esse método começa com todas as variáveis no modelo e remove-se uma a
uma aquelas que menos contribuem para a explicação da variável resposta,
com base em um critério estatístico.

**Passos:**

1.  Começa-se com todas as variáveis no modelo.

2.  Remove-se a variável com menor significância estatística (ex.: maior
    p-valor acima de um limiar, como 0.05).

3.  Reajusta-se o modelo e repete-se o processo até que todas as
    variáveis remanescentes sejam estatisticamente significativas.

## [Stepwise Selection (Seleção por Etapas)]{style="font-size: 40px;"}

Esse método combina Forward Selection e Backward Elimination. Ele
adiciona variáveis ao modelo, como no método forward, mas também
verifica se alguma variável já incluída pode ser removida.

**Passos:**

1.  Começa-se com um modelo vazio.

2.  Adiciona-se a variável mais significativa.

3.  Após cada inclusão, testa-se se alguma variável já incluída se
    tornou insignificante e, se for o caso, ela é removida.

4.  O processo continua até que não seja possível adicionar ou remover
    mais variáveis com base no critério escolhido.

## [Considerações sobre os critérios de seleção de variáveis]{style="font-size: 40px;"}

<br/>

-   Nenhum dos métodos garante encontrar o melhor subconjunto de
    variáveis globalmente ótimo.

<br/>

-   Critérios como o AIC, BIC ou $R^2$ ajustado podem ser usados para
    avaliar a qualidade dos modelos.

# Como escolher o melhor modelo ?

## [AIC (Critério de Informação de Akaike)]{style="font-size: 40px;"}

O **AIC** mede a qualidade relativa de um modelo estatístico,
penalizando a complexidade para evitar overfitting. Ele é definido como:

$$
AIC=−2\log(L)+2k
$$

Onde:

-   $L$ é a função de verossimilhança do modelo.

-   $K$ é o número de parâmetros estimados.

**Como interpretar?**

-   Modelos com **menor** AIC são preferíveis.

-   O AIC penaliza a complexidade, mas de forma **menos severa** que o
    BIC.

-   Muito usado em regressão e modelagem estatística, especialmente
    quando se busca comparar múltiplos modelos.

## [BIC (Critério de Informação Bayesiano)]{style="font-size: 40px;"}

O **BIC** é semelhante ao AIC, mas penaliza modelos mais complexos de
forma mais intensa. Sua fórmula é:

$$
BIC=−2\,\log(L)+k\,\log(n)
$$

Onde:

-   $N$ é o número de observações no conjunto de dados.

-   $K$ é o número de parâmetros do modelo.

**Como interpretar?**

-   Modelos com **menor** BIC são preferíveis.

-   Penaliza modelos com muitos parâmetros mais fortemente do que o AIC.

-   É mais rigoroso quando o tamanho da amostra nnn é grande.

-   Favorece modelos mais parcimoniosos, evitando overfitting.

## $R^2$ Ajustado

O $R^2$ ajustado é uma versão corrigida do coeficiente de determinação,
levando em conta a quantidade de variáveis no modelo. Sua fórmula é:

$$
R_{ajustado}^{2} =  1−\left( (1−R^2)\frac{(n−1)}{n−k−1} \right)
$$

Onde:

-   $R^2$ é o coeficiente de determinação tradicional.

-   $n$ é o número de observações.

-   $k$ é o número de variáveis independentes no modelo.

## [Como interpretar o $R^2$ Ajustado]{style="font-size: 40px;"}

<br/>

-   Indica a **proporção da variabilidade explicada** pelo modelo,
    ajustada pelo número de variáveis.

<br/>

-   Diferente do $R^2$ tradicional, ele **não aumenta necessariamente**
    com mais variáveis no modelo.

<br/>

-   Um $R^2$ ajustado **mais alto** sugere um modelo melhor, mas é
    importante compará-lo com outros critérios como AIC e BIC.

## Comparação e Uso Prático

<br/>

| Critério       | Penaliza Complexidade? | Aplicação                                    |
|------------------|-------------------|-----------------------------------|
| **AIC**        | Sim (moderado)         | Escolha de modelos estatísticos e regressões |
| **BIC**        | Sim (forte)            | Modelos bayesianos e regressão estatística   |
| $R^2$ Ajustado | Sim (implícito)        | Avaliação de regressão linear e predição     |


## Resumo dos critérios 

<br/>

-   O **AIC** e o **BIC** são ideais para **comparação de modelos**.

<br/>

-   O **BIC** é mais rigoroso na penalização de modelos complexos, sendo
    preferível quando se deseja simplicidade.

<br/>

-   O $R^2$ ajustado é útil para avaliar a qualidade de ajuste da
    regressão, mas não deve ser usado isoladamente para seleção de
    modelos.
    
    


